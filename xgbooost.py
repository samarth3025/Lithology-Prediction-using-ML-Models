# -*- coding: utf-8 -*-
"""XGBOOOST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHgRSk23jS9V7oObRzeufuKvak2eTLTc
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from mpl_toolkits.axes_grid1 import make_axes_locatable


plt.rcParams['font.size'] = 14
plt.rcParams['figure.figsize'] = (20, 16)
plt.rcParams['figure.facecolor'] = '#00000000'


from pandas import set_option
set_option("display.max_rows", 10)

filename = 'facies_dataset.csv'
training_data = pd.read_csv(filename)
training_data

# log data Preprocessing

facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',
       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']

facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',
                 'WS', 'D','PS', 'BS']
#facies_color_map is a dictionary that maps facies labels
#to their respective colors
facies_color_map = {}
for ind, label in enumerate(facies_labels):
    facies_color_map[label] = facies_colors[ind]
#this enumerate function is used to convert any list to a dictionary with index as its subject.

def label_facies(row, labels):
    return labels[ row['Facies'] -1]

training_data.loc[:,'FaciesLabels'] = training_data.apply(lambda row: label_facies(row, facies_labels), axis=1)

def make_facies_log_plot(logs, facies_colors):
    #make sure logs are sorted by depth
    logs = logs.sort_values(by='Depth')
    cmap_facies = colors.ListedColormap(
            facies_colors[0:len(facies_colors)], 'indexed')

    ztop=logs.Depth.min(); zbot=logs.Depth.max()

    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)

    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(8, 12))
    ax[0].plot(logs.GR, logs.Depth, '-g')
    ax[1].plot(logs.ILD_log10, logs.Depth, '-')
    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')
    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')
    ax[4].plot(logs.PE, logs.Depth, '-', color='black')
    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',
                    cmap=cmap_facies,vmin=1,vmax=9)

    divider = make_axes_locatable(ax[5])
    cax = divider.append_axes("right", size="20%", pad=0.05)
    cbar=plt.colorbar(im, cax=cax)
    cbar.set_label((17*' ').join([' SS ', 'CSiS', 'FSiS',
                                'SiSh', ' MS ', ' WS ', ' D  ',
                                ' PS ', ' BS ']))
    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')

    for i in range(len(ax)-1):
        ax[i].set_ylim(ztop,zbot)
        ax[i].invert_yaxis()
        ax[i].grid()
        ax[i].locator_params(axis='x', nbins=3)

    ax[0].set_xlabel("GR")
    ax[0].set_xlim(logs.GR.min(),logs.GR.max())
    ax[1].set_xlabel("ILD_log10")
    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())
    ax[2].set_xlabel("DeltaPHI")
    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())
    ax[3].set_xlabel("PHIND")
    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())
    ax[4].set_xlabel("PE")
    ax[4].set_xlim(logs.PE.min(),logs.PE.max())
    ax[5].set_xlabel('Facies')

    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])
    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])
    ax[5].set_xticklabels([])
    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)

facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D','PS', 'BS']

facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',
       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']search

#facies_color_map is a dictionary that maps facies labels to their respective colors

facies_color_map = {}
for ind, label in enumerate(facies_labels):
    facies_color_map[label] = facies_colors[ind]

def label_facies(row, labels):
    return labels[ row['Facies'] -1]

# train_df.loc[:,'FaciesLabels'] = train_df.apply(lambda row: label_facies(row, facies_labels), axis=1)
target = 'Facies'
# target classes
facies_counts = training_data[target].value_counts().sort_index()
facies_counts.index = facies_labels

# Data Preparation
# y value
correct_facies_labels = training_data['Facies'].values

# X value
feature_vectors = training_data.drop(['Formation', 'Well Name', 'Depth','Facies','FaciesLabels'], axis=1)

# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.metrics import classification_report, accuracy_score
# from scipy.stats import uniform, randint
# from sklearn.metrics import classification_report, accuracy_score

# X_train, X_test, y_train, y_test = train_test_split(feature_vectors, correct_facies_labels, test_size=0.2, random_state=42)

# ada_boost = AdaBoostClassifier(random_state=42)

# from sklearn.model_selection import RandomizedSearchCV
# #

# parameters = {
#     'n_estimators': randint(10, 60),
#     'learning_rate': uniform(0.01, 10)
# }

# random_search = RandomizedSearchCV(
#     estimator=ada_boost,
#     param_distributions=parameters,
#     n_iter=25,        # Number of parameter settings to sample
#     cv=5,             # Number of cross-validation folds
#     n_jobs=-1,        # Use all available cores
#     verbose=2,        # Verbosity level
#     random_state=42   # Seed for reproducibility
# )

# random_search.fit(X_train, y_train)

# best_ada_boost = random_search.best_estimator_
# y_pred = best_ada_boost.predict(X_test)
# print("Test set accuracy: ", accuracy_score(y_test, y_pred))
# print("Classification report:\n", classification_report(y_test, y_pred))

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from scipy.stats import uniform, randint
from sklearn.metrics import classification_report, accuracy_score

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
correct_facies_labels = encoder.fit_transform(correct_facies_labels)

xgb = XGBClassifier(random_state=42)

X_train, X_test, y_train, y_test = train_test_split(feature_vectors, correct_facies_labels, test_size=0.3, random_state=42)
from sklearn.preprocessing import LabelEncoder

# Convert target variable to zero-indexed labels
encoder = LabelEncoder()


param_distributions = {
    'n_estimators': randint(50, 500),               # Number of boosting rounds
    'learning_rate': uniform(0.01, 0.3),            # Step size shrinkage
    'max_depth': randint(3, 15),                    # Maximum depth of a tree
    'min_child_weight': randint(1, 10),             # Minimum sum of instance weight needed in a child
    'subsample': uniform(0.5, 0.5),                 # Proportion of samples used for fitting
    'colsample_bytree': uniform(0.5, 0.5),          # Proportion of features used for each tree
    'gamma': uniform(0, 0.5)                        # Minimum loss reduction required to make a further partition
}

random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_distributions,
    n_iter=20,          # Number of parameter settings to sample
    cv=5,               # Number of cross-validation folds
    n_jobs=-1,          # Use all available cores
    verbose=2,          # Verbosity level
    random_state=42     # Seed for reproducibility
)

# Fit to the data
random_search.fit(X_train, y_train)

print("Best parameters from Random Search:", random_search.best_params_)
print("Best score from Random Search:", random_search.best_score_)

best_model = random_search.best_estimator_

y_pred = best_model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# Precision (can specify the average type for multi-class problems)
precision = precision_score(y_test, y_pred, average='weighted')
print(f'Precision: {precision}')

# Recall (can specify the average type for multi-class problems)
recall = recall_score(y_test, y_pred, average='weighted')
print(f'Recall: {recall}')

# F1 Score (can specify the average type for multi-class problems)
f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1 Score: {f1}')

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)